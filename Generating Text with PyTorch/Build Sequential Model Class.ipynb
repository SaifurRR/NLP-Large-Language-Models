{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9378e93",
   "metadata": {
    "editable": false,
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752cd870",
   "metadata": {},
   "source": [
    "**Dataset Cleaning, Tokenization, and Preprocessing**\n",
    "\n",
    "Run the setup cell below which has opened and read the ebook of [_Pride and Prejudice_](https://www.gutenberg.org/files/1342/1342-h/1342-h.htm) to the variable `raw_text`. \n",
    "\n",
    "**Note**: Due to hardware constraints, we'll only use the full text from **Chapter 1** which we've indexed and saved to the variable `raw_text_ch1`.\n",
    "\n",
    "We've cleaned and tokenized the text to individual word-based tokens into the following variables:\n",
    "- `lowered_text` : contains the full raw text where every character is lowercased\n",
    "- `preprocessed_text` : contains the lowercased text where punctuation marks and special characters are removed\n",
    "- `tokenized_text` : contains the full text tokenized as a list of word-based tokens\n",
    "\n",
    "We've also created the vocabularies and obtained the vocabulary size saved as the following variables:\n",
    "- `w2ix` : vocabulary mapping tokens to their assigned token ID\n",
    "- `vocab_size` : the vocabulary size of `w2ix`\n",
    "- `ix2w` : inverse vocabulary mapping token IDs back to its word-based token\n",
    "\n",
    "Using the vocabulary, we created the variables:\n",
    "- `tokenized_id_text` : the tokenized text of Chapter 1 mapped to token IDs\n",
    "\n",
    "Lastly, the bigrams were created into features and labels and saved as the following variables:\n",
    "- `bigrams_ch1` : contains the bigram pairs as token IDs for the tokenized text of Chapter 1\n",
    "- `features` : contains the context token for each bigram\n",
    "- `labels` : contains the target token for each bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5210f6d",
   "metadata": {
    "editable": false,
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "with open('datasets/book.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# index Chapter 1\n",
    "raw_text_ch1 = raw_text[1985:6468]\n",
    "\n",
    "# cleaning and tokenization\n",
    "lowered_text = raw_text_ch1.lower()\n",
    "preprocessed_text = re.sub(r'([,.:;?_&$!\"()\\-\\*\\']|--|)', '', lowered_text)\n",
    "tokenized_text = preprocessed_text.split()\n",
    "\n",
    "# create vocabularies\n",
    "unique_tokens = sorted(list(set(tokenized_text)))\n",
    "w2ix = {word:ix for ix, word in enumerate(unique_tokens)}\n",
    "vocab_size = len(w2ix)\n",
    "ix2w = {ix:word for word,ix in w2ix.items()}\n",
    "\n",
    "# token ID mapping of the text\n",
    "tokenized_id_text = [w2ix[word] for word in tokenized_text]\n",
    "\n",
    "# Map the tokens in the text to their token IDs\n",
    "tokenized_id_text = [w2ix[word] for word in tokenized_text]\n",
    "\n",
    "# Create Bigrams\n",
    "bigrams_ch1 = np.array([[tokenized_id_text[i], \n",
    "                          tokenized_id_text[i + 1]] \n",
    "                          for i in range(len(tokenized_id_text) - 1)])\n",
    "\n",
    "# Create Features and Labels Tensors\n",
    "features = torch.tensor(bigrams_ch1[:,0],dtype=torch.long)\n",
    "labels = torch.tensor(bigrams_ch1[:,1],dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c74d0",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We've started constructing the `NextWordBigram` class for our bigram model from the narrative.\n",
    "\n",
    "Complete the `__init__()` and `forward()` methods with the following architecture:\n",
    "\n",
    "1. In the `__init__()` method, initialize the following layers:\n",
    "    - `self.embedding` for the embedding layer where the number of embeddings is equal to the **vocabulary size** making sure each embedding contains **2** dimensions\n",
    "    - `self.linear1` for the first linear layer with an input size of **2** and an output size of **18**\n",
    "    - `self.linear2` for the second linear layer with an input size of **18** and an output size equal to the **vocabulary size**\n",
    "    \n",
    "\n",
    "2. In the `forward()` method, create the forward operations starting with the input `x` in the following order: \n",
    "    1. `self.embedding`\n",
    "    2. `self.linear1`\n",
    "    3. `self.linear2`\n",
    "    4. return the output from `self.linear2`\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b318baec",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "cp1"
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "torch.manual_seed(1) # set random seed --do not change!\n",
    "\n",
    "class NextWordBigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NextWordBigram, self).__init__()\n",
    "        ## YOUR SOLUTION HERE ##\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                      embedding_dim=2)\n",
    "        self.linear1 = nn.Linear(2,18)\n",
    "        self.linear2 = nn.Linear(18, vocab_size)                \n",
    "        \n",
    "    def forward(self,x):\n",
    "        ## YOUR SOLUTION HERE ##\n",
    "        x=self.embedding(x)\n",
    "        x=self.linear1(x)\n",
    "        x=self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a172e5e",
   "metadata": {},
   "source": [
    "Next, let's create an instance of the `NextWordBigram` class and save it to the variable `bigram_model`.\n",
    "\n",
    "Be sure to set the model to **evaluation mode**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e73ed471",
   "metadata": {
    "deletable": false,
    "tags": [
     "cp2"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NextWordBigram(\n",
       "  (embedding): Embedding(321, 2)\n",
       "  (linear1): Linear(in_features=2, out_features=18, bias=True)\n",
       "  (linear2): Linear(in_features=18, out_features=321, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model = NextWordBigram()\n",
    "bigram_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f429ab3-f1e0-443d-a552-39effbbf7034",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "Lastly, let's generate some text from the untrained bigram model!\n",
    "\n",
    "Let's see if the model can re-create the first sentence in Chapter 1: \n",
    "\n",
    "```md\n",
    "it is a truth universally acknowledged that a single man in possession of a good fortune, must be in want of a wife\n",
    "```\n",
    "\n",
    "We've provided the first four tokens `['it', 'is', 'a', 'truth']` as the starting prompt. The bigram model will generate (predict) the next `10` tokens starting with the last token `'truth'` as the first context token.\n",
    "\n",
    "Create a loop that generates the next 10 tokens:\n",
    "1. Select the last token as the context token\n",
    "2. Input the context token through the forward pass of the model to generate token scores\n",
    "3. Use `torch.argmax` to select the token ID with the highest score (the predicted token)\n",
    "4. Convert the predicted token ID to the actual token using the reverse vocabulary `ix2w`\n",
    "5. Append the generated token to the starting prompt\n",
    "\n",
    "Join the tokens in the starting prompt using `' '.join()` and save it to the variable `generated_text`.\n",
    "\n",
    "Print `generated_text`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ddf801b",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "cp3"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is a truth bit high so bit high so bit high so bit\n"
     ]
    }
   ],
   "source": [
    "starting_prompt = ['it', 'is', 'a', 'truth']\n",
    "num_generated_tokens = 10\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_generated_tokens):\n",
    "        context_token=torch.tensor(w2ix[starting_prompt[-1]], dtype=torch.long)\n",
    "        token_scores = bigram_model(context_token)\n",
    "        predicted_token_id =torch.argmax(token_scores, dim=0).item()\n",
    "        predicted_token=ix2w[predicted_token_id]\n",
    "        starting_prompt.append(predicted_token)\n",
    "        \n",
    "generated_text = ' '.join(starting_prompt)\n",
    "\n",
    "# show output - generated text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d4b23-fb08-4270-9fa7-5b49e23af5e1",
   "metadata": {},
   "source": [
    "**Explaining the text generation output**\n",
    "\n",
    "The text generated from the untrained model doesn't make too much sense because it seems to be repeating the tokens `'hope '`, and `'impatiently'`. \n",
    "\n",
    "Here is an explanation of how the bigram model generates a new token at each iteration:\n",
    "- the first context token `'truth'` is predicted to have the next token `'hope'`\n",
    "- the previously predicted token `'hope'` becomes the next context token which is used to predict the next token `'impatiently'`\n",
    "- the previously predicted token  `'impatiently'` becomes the next context token which is used to predict the next token `'hope'`\n",
    "    - this causes the repeated predictions `'hope' ==> 'impatiently' ==> 'hope ' ==> 'impatiently'`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
