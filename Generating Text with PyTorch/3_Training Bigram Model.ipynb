{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9378e93",
   "metadata": {
    "editable": false,
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba954ea7-b2a8-4bd5-a362-6cda14265b70",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Dataset Cleaning, Tokenization, and Preprocessing**\n",
    "\n",
    "Run the setup cell below which has opened and read the ebook of [_Pride and Prejudice_](https://www.gutenberg.org/files/1342/1342-h/1342-h.htm) to the variable `raw_text`. \n",
    "\n",
    "**Note**: Due to hardware constraints, we'll only use the full text from **Chapter 1** which we've indexed and saved to the variable `raw_text_ch1`.\n",
    "\n",
    "We've cleaned and tokenized the text to individual word-based tokens into the following variables:\n",
    "- `lowered_text` : contains the full raw text where every character is lowercased\n",
    "- `preprocessed_text` : contains the lowercased text where punctuation marks and special characters are removed\n",
    "- `tokenized_text` : contains the full text tokenized as a list of word-based tokens\n",
    "\n",
    "We've also created the vocabularies and obtained the vocabulary size saved as the following variables:\n",
    "- `w2ix` : vocabulary mapping tokens to their assigned token ID\n",
    "- `vocab_size` : the vocabulary size of `w2ix`\n",
    "- `ix2w` : inverse vocabulary mapping token IDs back to its word-based token\n",
    "\n",
    "Using the vocabulary, we created the variables:\n",
    "- `tokenized_id_text` : the tokenized text of Chapter 1 mapped to token IDs\n",
    "\n",
    "Lastly, the bigrams were created into features and labels and saved as the following variables:\n",
    "- `bigrams_ch1` : contains the bigram pairs as token IDs for the tokenized text of Chapter 1\n",
    "- `features` : contains the context token for each bigram\n",
    "- `labels` : contains the target token for each bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1293137e-6d05-4c7b-af38-76163586390b",
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "with open('datasets/book.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# index Chapter 1\n",
    "raw_text_ch1 = raw_text[1985:6468]\n",
    "\n",
    "# cleaning and tokenization\n",
    "lowered_text = raw_text_ch1.lower()\n",
    "preprocessed_text = re.sub(r'([,.:;?_&$!\"()\\-\\*\\']|--|)', '', lowered_text)\n",
    "tokenized_text = preprocessed_text.split()\n",
    "\n",
    "# create vocabularies\n",
    "unique_tokens = sorted(list(set(tokenized_text)))\n",
    "w2ix = {word:ix for ix, word in enumerate(unique_tokens)}\n",
    "vocab_size = len(w2ix)\n",
    "ix2w = {ix:word for word,ix in w2ix.items()}\n",
    "\n",
    "# token ID mapping of the text\n",
    "tokenized_id_text = [w2ix[word] for word in tokenized_text]\n",
    "\n",
    "# Map the tokens in the text to their token IDs\n",
    "tokenized_id_text = [w2ix[word] for word in tokenized_text]\n",
    "\n",
    "# Create Bigrams\n",
    "bigrams_ch1 = np.array([[tokenized_id_text[i], \n",
    "                          tokenized_id_text[i + 1]] \n",
    "                          for i in range(len(tokenized_id_text) - 1)])\n",
    "\n",
    "# Create Features and Labels Tensors\n",
    "features = torch.tensor(bigrams_ch1[:,0],dtype=torch.long)\n",
    "labels = torch.tensor(bigrams_ch1[:,1],dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dd4f1c-5718-46d9-93d4-33a93752f4c2",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Construct Bigram Model Architecture**\n",
    "\n",
    "Run the setup up cell below to create the `NextWordBigram` model class we built earlier that has been instantiated to the variable `bigram_model`. \n",
    "\n",
    "**Note**: we've slightly modified the the embedding layer to create embeddings with a dimension size of 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58663451-17d8-474f-ba49-ec3802104d11",
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "torch.manual_seed(1) # set random seed --do not change!\n",
    "\n",
    "class NextWordBigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NextWordBigram, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, 6)\n",
    "        self.layer1 = nn.Linear(6, 18)\n",
    "        self.layer2 = nn.Linear(18, vocab_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the bigram model\n",
    "bigram_model = NextWordBigram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e0e0c",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "First, let's initialize the loss function and the optimizer for training.\n",
    "\n",
    "1. Create an instance of the cross-entropy loss function for multiclass classification and save it to the variable `loss`.\n",
    "\n",
    "2. Create an instance of the Adam optimizer with a learning rate of `0.01` and save it to the variable `optimizer`.\n",
    "\n",
    "**Note**: Be sure to run the three **Setup** cells above (importing libraries, preprocessing the dataset, and constructing the bigram architecture) before completing the checkpoints!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b318baec",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "cp1"
    ]
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "torch.manual_seed(1) # set random seed --do not change!\n",
    "\n",
    "## YOUR SOLUTION HERE ##\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(bigram_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6079f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a172e5e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "Let's now create the training loop that consists of training the network for `700` epochs with the following steps at each iteration:\n",
    "\n",
    "1. Reset the gradients\n",
    "2. Apply the forward pass\n",
    "3. Compute the loss\n",
    "4. Compute the gradients\n",
    "5. Update the weights and biases \n",
    "\n",
    "Keep track of the loss during training by printing out the loss every 100 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e73ed471",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "cp2"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/700], CELoss: 1.9768\n",
      "Epoch [200/700], CELoss: 1.4131\n",
      "Epoch [300/700], CELoss: 1.3493\n",
      "Epoch [400/700], CELoss: 1.3399\n",
      "Epoch [500/700], CELoss: 1.3375\n",
      "Epoch [600/700], CELoss: 1.3377\n",
      "Epoch [700/700], CELoss: 1.3361\n"
     ]
    }
   ],
   "source": [
    "# initialize model and model components -- do not change!\n",
    "torch.manual_seed(1)\n",
    "bigram_model = NextWordBigram()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(bigram_model.parameters(), lr=0.01)\n",
    "\n",
    "## YOUR SOLUTION HERE ##\n",
    "num_epochs = 700\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = bigram_model(features)\n",
    "    CEloss = loss(predictions, labels) #CrossEntropyLoss\n",
    "    CEloss.backward() #compute gradients\n",
    "    optimizer.step()  # update weights and biases      \n",
    "        \n",
    "    \n",
    "    ## DO NOT MODIFY ##\n",
    "    # keep track of the loss during training every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], CELoss: {CEloss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d481799",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "Let's now generate text from the trained bigram model!\n",
    "\n",
    "We've already set the model to evaluation mode and provided the same starting prompt as before: `['it', 'is', 'a', 'truth']` to see if our model can re-create the first sentence in our text: \n",
    "\n",
    "```md\n",
    "it is a truth universally acknowledged that a single man in possession of a good fortune, must be in want of a wife\n",
    "```\n",
    "\n",
    "Create a loop that generates the next `20` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ddf801b",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "cp3"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is a truth universally acknowledged that he is not you must know mrs long says that he is not you must know mrs\n"
     ]
    }
   ],
   "source": [
    "# set the model to evaluation mode\n",
    "bigram_model.eval()\n",
    "\n",
    "starting_prompt = ['it', 'is', 'a', 'truth']\n",
    "num_generated_tokens = 20 \n",
    "\n",
    "for _ in range(num_generated_tokens):\n",
    "    ## YOUR SOLUTION HERE ##    \n",
    "    context_token=torch.tensor(w2ix[starting_prompt[-1]], dtype=torch.long)\n",
    "    token_scores = bigram_model(context_token)\n",
    "    predicted_token_id =torch.argmax(token_scores, dim=0).item()\n",
    "    predicted_token=ix2w[predicted_token_id]\n",
    "    starting_prompt.append(predicted_token)\n",
    "        \n",
    "generated_text = ' '.join(starting_prompt)\n",
    "     \n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b21ecc-bf83-4a5a-8996-92a0831c95e3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Nice, it looks like the trained bigram model generates slightly more coherent text than the untrained bigram model. \n",
    "\n",
    "Specifically, it correctly predicted the next three tokens: `'universally'`, `'acknowledged'`, and `'that'`. \n",
    "\n",
    "However, we see the limitations of the bigram model as it quickly falls into repetition, repeating the sequence `'that he is not you must know mrs'`. Since bigram models only use a single context token (the previous token) to predict the next token, it won't be able to learn the long-term contextual relationships within the text beyond those immediate bigram pairs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246dc643-8327-4a1b-bb75-fa3dc05e6c30",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Calculate Next Word Probabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1c02616-f219-4cbb-bde4-257113bbd3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Next Token</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>universally</td>\n",
       "      <td>0.50003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>is</td>\n",
       "      <td>0.49978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>when</td>\n",
       "      <td>0.00016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>these</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>preference</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Next Token  Probability\n",
       "289  universally      0.50003\n",
       "130           is      0.49978\n",
       "305         when      0.00016\n",
       "271        these      0.00002\n",
       "219   preference      0.00000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_token = 'truth'\n",
    "index = torch.tensor(w2ix[context_token],dtype=torch.long)\n",
    "predict = torch.softmax(bigram_model(index), dim=0)\n",
    "\n",
    "data = []\n",
    "for ix, item in enumerate(predict):\n",
    "    data.append([ix2w[ix], round(item.item(), 5)])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=['Next Token', 'Probability'])\n",
    "df.sort_values(\"Probability\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87857d18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
