{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9378e93",
   "metadata": {
    "editable": false,
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377d8d66-1eb6-467d-ae1e-dffdd4d6e2c3",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "Run the cell below to open and read the ebook of [_Pride and Prejudice_](https://www.gutenberg.org/files/1342/1342-h/1342-h.htm) to the variable `raw_text`. \n",
    "\n",
    "**Note**: Due to hardware constraints, we'll only use the full text from **Chapter 1** which we've indexed and saved to the variable `raw_text_ch1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5210f6d",
   "metadata": {
    "editable": false,
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune, must be in want of a wife.\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/book.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# index chapter 1\n",
    "raw_text_ch1 = raw_text[1985:6468]\n",
    "print(raw_text_ch1[:117])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e9726-9853-4fee-9941-9ff143814a11",
   "metadata": {},
   "source": [
    "**Tokenization and Preprocessing**\n",
    "\n",
    "We've tokenized and preprocessed the raw text from Chapter 1 into the following variables:\n",
    "\n",
    "- `tokenized_text` : contains the full raw text tokenized as a list of character-based tokens\n",
    "- `c2ix` : contains the vocabulary of unique character tokens mapped to their unique token IDs\n",
    "- `vocab_size` : is the the vocabulary size\n",
    "- `ix2c` : contains the inverse vocabulary of unique token IDs mapped to their unique character tokens\n",
    "- `tokenized_id_text` maps the tokens in the tokenized text to their token IDs\n",
    "\n",
    "We also used the `Dataset` and `DataLoader` utility classes to create the following variables:\n",
    "- `dataset` : stores and creates the sequences for the features and labels with a sequence length of `24`\n",
    "- `dataloader` : contains the iterable used to load the sequences as batches with a batch size of `48`\n",
    "\n",
    "Run the cell below to tokenize and preprocess the raw text from Chapter 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b34a572-5925-4778-a8e3-263c13d49b46",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "tokenized_text = list(raw_text_ch1)\n",
    "unique_character_tokens = sorted(list(set(tokenized_text)))\n",
    "c2ix = {ch:i for i,ch in enumerate(unique_character_tokens)}\n",
    "vocab_size = len(c2ix)\n",
    "ix2c = {ix:ch for ch,ix in c2ix.items()}\n",
    "\n",
    "tokenized_id_text = [c2ix[ch] for ch in tokenized_text]\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.manual_seed(1) # set random seed --do not change!\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenized_text, seq_length):\n",
    "        self.tokenized_text = tokenized_text\n",
    "        self.seq_length = seq_length\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_text) - self.seq_length\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self.tokenized_text[idx:idx+self.seq_length])\n",
    "        labels = torch.tensor(self.tokenized_text[idx+1:idx+self.seq_length+1])\n",
    "        return features, labels\n",
    "                \n",
    "seq_length = 24\n",
    "batch_size = 48\n",
    "dataset = TextDataset(tokenized_id_text, seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75416ff3-9436-4eda-bdc0-a9f8d4d43b1e",
   "metadata": {},
   "source": [
    "**Construct the Character-Based LSTM Class**\n",
    "\n",
    "We've constructed the Character-Based LSTM class based on the previous exercise. \n",
    "\n",
    "We run the cell below to create an instance of the LSTM class saved to the variable `lstm_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6583d5f-9c41-47e8-9163-43a7d3e66a40",
   "metadata": {
    "editable": false,
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "torch.manual_seed(1) # set random seed --do not change!\n",
    "\n",
    "class CharacterLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CharacterLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=64)\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=128, batch_first=True)\n",
    "        self.linear = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, x, states):\n",
    "        x = self.embedding(x)\n",
    "        out, states = self.lstm(x, states)\n",
    "        out = self.linear(out)\n",
    "        out = out.reshape(-1, out.size(2))\n",
    "        return out, states\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, 128)\n",
    "        cell = torch.zeros(1, batch_size, 128)\n",
    "        return hidden, cell\n",
    "\n",
    "lstm_model = CharacterLSTM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c74d0",
   "metadata": {},
   "source": [
    "Using the created instance of the LSTM class `lstm_model`, let's set up the loss function and optimizer to train our model:\n",
    "\n",
    "1. Create an instance of the **multiclass cross-entropy** loss function and save it to the variable `loss`.\n",
    "\n",
    "2. Create an instance of the **Adam** optimizer with a learning rate of `0.01` and save it to the variable `optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b318baec",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "cp1"
    ]
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "torch.manual_seed(1) # set random seed \n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a172e5e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now let's train the LSTM by implementing the training loop. We've started building the loop that trains the model for 10 epochs where in each epoch, we iterate through all of the batches in the `dataloader` to train the model one batch at a time.\n",
    "\n",
    "Complete the training loop that trains the model one batch at a time with the following:\n",
    "1. Reset the gradients\n",
    "2. Reset the hidden and cell states\n",
    "3. Apply the forward pass (that returns the output and updates the states)\n",
    "4. Calculate the loss\n",
    "5. Compute the gradients\n",
    "6. Update the weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e73ed471",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "cp2"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], CELoss: 1.3437\n",
      "Epoch [2/10], CELoss: 0.7308\n",
      "Epoch [3/10], CELoss: 0.5384\n",
      "Epoch [4/10], CELoss: 0.4454\n",
      "Epoch [5/10], CELoss: 0.4315\n",
      "Epoch [6/10], CELoss: 0.3848\n",
      "Epoch [7/10], CELoss: 0.3613\n",
      "Epoch [8/10], CELoss: 0.3683\n",
      "Epoch [9/10], CELoss: 0.3468\n",
      "Epoch [10/10], CELoss: 0.3214\n"
     ]
    }
   ],
   "source": [
    "# initialize model and model components \n",
    "lstm_model = CharacterLSTM()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for features, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        states = lstm_model.init_state(features.size(0))\n",
    "        outputs, states = lstm_model(features, states)\n",
    "        CEloss = loss(outputs, labels.view(-1))\n",
    "        CEloss.backward()\n",
    "        optimizer.step()        \n",
    "    # keep track of the loss during training\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], CELoss: {CEloss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d481799",
   "metadata": {},
   "source": [
    "Let's see if the trained LSTM can generate the first sentence of our text in Chapter 1: \n",
    "\n",
    "```md\n",
    "It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\n",
    "```\n",
    "\n",
    "We'll use the starting prompt `\"It is a truth\"` that's been tokenized to its character-based token IDs saved to the variable `starting_prompt`.\n",
    "\n",
    "We've also set the model to evaluation mode and specified that we want to generate `250` new characters.\n",
    "\n",
    "Within the `torch.no_grad():` context, we initialized the clean hidden and cell states to the variable `states`. \n",
    "\n",
    "We finish creating the `for` loop that generates one character per iteration with the following:\n",
    "1. Input the tokenized prompt through the forward pass to generate the output and updated states\n",
    "2. Use `torch.argmax` to select the token ID with the highest output score\n",
    "3. Use the inverse vocabulary `ix2c` to map the selected token ID to its character-based token\n",
    "4. Append the generated token to the starting prompt\n",
    "5. Update the tokenized prompt with the newly generated token\n",
    "\n",
    "Lastly we print the starting prompt with the newly generated text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ddf801b",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "cp3"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a truth universally acknowledged, that a single man in possession\n",
      "of a good fortune, must be in want of a wife.\n",
      "\n",
      "However little known the feelings or views of such a man may be on his\n",
      "first entering a neighbourhood.\"\n",
      "\n",
      "\"It will be no use to us, if twenty suc\n"
     ]
    }
   ],
   "source": [
    "starting_prompt = \"It is a truth\"\n",
    "tokenized_id_prompt = torch.tensor([[c2ix[ch] for ch in starting_prompt]])\n",
    "\n",
    "lstm_model.eval()\n",
    "num_generated_chars = 250\n",
    "with torch.no_grad():\n",
    "    states = lstm_model.init_state(1)\n",
    "    for _ in range(num_generated_chars):\n",
    "        output, states = lstm_model(tokenized_id_prompt, states)\n",
    "        predicted_id = torch.argmax(output[-1, :], dim=-1).item()\n",
    "        predicted_char = ix2c[predicted_id]\n",
    "        starting_prompt += predicted_char\n",
    "        tokenized_id_prompt = torch.tensor([[predicted_id]])        \n",
    "\n",
    "# print the generated text\n",
    "print(starting_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafa2cb5-4ebe-444f-83ad-e9a20a5174ee",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The LSTM model was able to successfully generate the full first sentence: `It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.`\n",
    "\n",
    "Notably, it is even able to correctly generate the comma `','` after the word `'acknowledged'`. This is because we decided to _not_ remove punctuations and special characters from the raw text which allowed the LSTM to learn to generate them instead!\n",
    "\n",
    "Afterward, it starts to deviate from the actual text but still maintains some grammatical accuracy!\n",
    "\n",
    "Remember, our model was trained fairly shortly and only on a small portion (Chapter 1) of the full text!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933acb6c-7e56-45fd-a7db-e7296a4d3588",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Tips to improve the model**\n",
    "\n",
    "Here are some ways to further improve our text generation model:\n",
    "- use the full text (or gather multiple outside texts)\n",
    "- use a larger embedding size (GPT3 uses a dimension size of ~12,000!)\n",
    "- modify the neural network architecture (add more neurons, layers, activation functions, etc.)\n",
    "- increase the number of epochs for training\n",
    "- test different optimizers and learning rates\n",
    "\n",
    "Unfortunately, due to the hardware constraints in our environments, we won't have the computation power to train larger networks with larger datasets without crashing our notebook. But feel free to build, train, and test a language model on your own!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
