{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9378e93",
   "metadata": {
    "editable": false,
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752cd870",
   "metadata": {},
   "source": [
    "**Dataset Cleaning and Tokenization**\n",
    "\n",
    "Run the setup cell below which has opened and read the ebook of [_Pride and Prejudice_](https://www.gutenberg.org/files/1342/1342-h/1342-h.htm) to the variable `raw_text`. \n",
    "\n",
    "**Note**: Due to hardware constraints, we'll only use the full text from **Chapter 1** which we've indexed and saved to the variable `raw_text_ch1`.\n",
    "\n",
    "We've cleaned and tokenized the text to individual word-based tokens into the following variables:\n",
    "- `lowered_text` : contains the full raw text where every character is lowercased\n",
    "- `preprocessed_text` : contains the lowercased text where punctuation marks and special characters are removed\n",
    "- `tokenized_text` : contains the full text tokenized as a list of word-based tokens\n",
    "\n",
    "We've also created the vocabularies and obtained the vocabulary size saved as the following variables:\n",
    "- `w2ix` : vocabulary mapping tokens to their assigned token ID\n",
    "- `vocab_size` : the vocabulary size of `w2ix`\n",
    "- `ix2w` : inverse vocabulary mapping token IDs back to its word-based token\n",
    "\n",
    "Using the vocabulary, we created the variable:\n",
    "- `tokenized_id_text` : the tokenized text of Chapter 1 mapped to token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5210f6d",
   "metadata": {
    "editable": false,
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size (Chapter 1): 321\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/book.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# index Chapter 1\n",
    "raw_text_ch1 = raw_text[1985:6468]\n",
    "\n",
    "# cleaning and tokenization\n",
    "lowered_text = raw_text_ch1.lower()\n",
    "preprocessed_text = re.sub(r'([,.:;?_&$!\"()\\-\\*\\']|--|)', '', lowered_text)\n",
    "tokenized_text = preprocessed_text.split()\n",
    "\n",
    "# create vocabularies\n",
    "unique_tokens = sorted(list(set(tokenized_text)))\n",
    "w2ix = {word:ix for ix, word in enumerate(unique_tokens)}\n",
    "vocab_size = len(w2ix)\n",
    "ix2w = {ix:word for word,ix in w2ix.items()}\n",
    "\n",
    "# token ID mapping of the text\n",
    "tokenized_id_text = [w2ix[word] for word in tokenized_text]\n",
    "\n",
    "print(\"Vocabulary Size (Chapter 1):\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c8532f",
   "metadata": {},
   "source": [
    "\n",
    "The vocabulary size of Chapter 1 is **321** which is significantly lower than the vocabulary size of the full text (which was 7338). Again, the reason we'll only be using the text from Chapter 1 is due to hardware constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c74d0",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's now create bigrams for the tokenized text of Chapter 1.\n",
    "\n",
    "Using the tokenized sentence of token IDs, create a NumPy array of bigrams where for each bigram pair:\n",
    "- the first token is the token ID of the context token\n",
    "- the second token is the token ID of the target token\n",
    "\n",
    "Save the bigrams to the variable `bigrams_ch1`. \n",
    "\n",
    "Print out the number of bigrams created, as well as, the first 10 bigrams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b318baec",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "cp1"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bigrams in Chapter 1: 854\n",
      "First 5 bigrams: \n",
      " [[131 130]\n",
      " [130   0]\n",
      " [  0 284]\n",
      " [284 289]\n",
      " [289   4]\n",
      " [  4 264]\n",
      " [264   0]\n",
      " [  0 244]\n",
      " [244 156]\n",
      " [156 124]]\n"
     ]
    }
   ],
   "source": [
    "bigrams_ch1 = np.array([[tokenized_id_text[i], tokenized_id_text[i+1]] for i in range(len(tokenized_id_text)-1)])\n",
    "\n",
    "# show output - number of bigrams and first 10 bigrams\n",
    "print(\"Number of bigrams in Chapter 1:\", len(bigrams_ch1))\n",
    "print(\"First 5 bigrams: \\n\", bigrams_ch1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a172e5e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Next, let's preprocess the bigrams in Chapter 1 into the following PyTorch tensors:\n",
    "- `features`: contains the context tokens\n",
    "- `labels`: contains the target tokens\n",
    "\n",
    "Be sure to specify the `torch.long` datatype for both tensors.\n",
    "\n",
    "Print out the first 10 values in `features` and `labels`. These should match the bigram pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e73ed471",
   "metadata": {
    "deletable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "cp2"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 features: tensor([131, 130,   0, 284, 289,   4, 264,   0, 244, 156])\n",
      "First 10 labels: tensor([130,   0, 284, 289,   4, 264,   0, 244, 156, 124])\n"
     ]
    }
   ],
   "source": [
    "features = torch.tensor(bigrams_ch1[:,0])\n",
    "labels = torch.tensor(bigrams_ch1[:,1])\n",
    "\n",
    "#print the first 10 features and labels\n",
    "print(\"First 10 features:\", features[:10])\n",
    "print(\"First 10 labels:\", labels[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
